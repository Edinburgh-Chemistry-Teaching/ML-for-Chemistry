{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159985b4",
   "metadata": {},
   "source": [
    "## Assessment Input Data cleaning\n",
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: \n",
    "- Dr Antonia Mey (antonia.mey@ed.ac.uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e53ff",
   "metadata": {},
   "source": [
    "## 1. The Dataset\n",
    "\n",
    "You will be working with the 'QMrxn20: Thousands of reactants and transition states for competing E2 and SN2 reactions' dataset. The whole dataset can be found here: [https://archive.materialscloud.org/record/2020.55](https://archive.materialscloud.org/record/2020.55).\n",
    "\n",
    "Below the data has been prepped already for machine learning for you. Some of the original data is included in the notebook for you to explore the data. This includes a file called `energies.txt` which shows a subset of the dataset and contains effectively a table  with referencing molecular geometries files and a label if it is in a transition state or not. \n",
    "\n",
    "```\n",
    "label,reaction,geometry,number,filename,energy,method\n",
    "A_A_A_A_A_A,e2,ts,0,transition-states/e2/A_A_A_A_A_A.xyz,-179.132058577095,mp2\n",
    "A_A_A_A_B_A,e2,ts,0,transition-states/e2/A_A_A_A_B_A.xyz,-539.161015594451,mp2\n",
    "A_A_A_A_B_B,e2,ts,0,transition-states/e2/A_A_A_A_B_B.xyz,-638.348834529304,mp2\n",
    "A_A_A_A_B_C,e2,ts,0,transition-states/e2/A_A_A_A_B_C.xyz,-998.3806344381261,mp2\n",
    "A_A_A_A_B_D,e2,ts,0,transition-states/e2/A_A_A_A_B_D.xyz,-3111.578751937588,mp2\n",
    "A_A_A_A_C_A,e2,ts,0,transition-states/e2/A_A_A_A_C_A.xyz,-2652.360661690992,mp2\n",
    "```\n",
    "\n",
    "Some of the transition state geometries with `xyz` files are also included. In the first part you are encouraged to explore the dataset a little bit, in conjunction with writing an introduction. \n",
    "\n",
    "### External references\n",
    "**Journal reference**\n",
    "G. F. von Rudorff, S. N. Heinen, M. Bragato, O. A. von Lilienfeld, Machine Learning: Science and Technology 1, 045026 (2020). doi:10.1088/2632-2153/aba822    \n",
    "**Preprint (Preprint where the data generation is discussed)**\n",
    "G. F. von Rudorff, S. N. Heinen, M. Bragato, O. A. von Lilienfeld, arXiv:2006.00504"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00750f53",
   "metadata": {},
   "source": [
    "## 2. Sub-sampling the dataset\n",
    "We don't need over 400k input data for a classification model, so we can easily subsample the data to 80 k structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = pd.read_csv('data/energies.txt')\n",
    "subsampled_energies = energies.iloc[::5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the subsamples\n",
    "subsampled_energies.to_csv('data/subsampled_energies.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cbc09",
   "metadata": {},
   "source": [
    "## 3. Putting coordinates and labels into one file\n",
    "Here we extract the xyz coordiantes from the xyz files and write everything into one big csv file which is the basis of the input dataset for the assessment.\n",
    "\n",
    "You will need the xyz files from the dataset for this all place in the `data` subdirectory. The list of directories is:\n",
    "\n",
    "```bash\n",
    "product-conformers \n",
    "reactant-complex-constrained-conformers \n",
    "reactant-complex-unconstrained-conformers\n",
    "reactant-conformers\n",
    "transition-states\n",
    "```\n",
    "\n",
    "Please download them separately from here, as they are too large for the GitHub repo. \n",
    "They can be found here: [https://archive.materialscloud.org/record/2020.55](https://archive.materialscloud.org/record/2020.55)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eda3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='data/'\n",
    "energies = pd.read_csv('data/subsampled_energies.txt')\n",
    "\n",
    "# get coordinates of all atoms i\n",
    "#option: make 3 columns with tuples i.e. x col contains all x coordinates etc ??\n",
    "\n",
    "def get_coordinates(n, energies, cols, all_cols):\n",
    "    # for row n in energies file ...\n",
    "    coord_path = path + energies['filename'][n]\n",
    "    # print(coord_path)\n",
    "    # read coordinates\n",
    "    coords = pd.read_csv(coord_path, skiprows = 1, delim_whitespace = True)\n",
    "    # make temporary df with coordinates for each atom\n",
    "    coord_df = pd.DataFrame(data = coords.values, columns = cols)\n",
    "    # if there are <17 atoms in compound then pack left over columns with nan\n",
    "    buffer = np.empty((1,4*(21 - len(coord_df.index)))).reshape(1,-1)\n",
    "    buffer[:] = np.nan\n",
    "    data = np.concatenate((coord_df.values.reshape((1,-1)), buffer), axis = 1)\n",
    "    # make df containing all coordinates \n",
    "    df_line = pd.DataFrame(data, columns = all_cols)\n",
    "    # df.drop(columns='filename',inplace=True)\n",
    "    return df_line\n",
    "\n",
    "cols = ['element','x coordinates', 'y coordinates', 'z coordinates'] \n",
    "# make enough columns to store all coordinates \n",
    "all_cols = [x+'_'+str(i) for i in range(21) for x in cols]\n",
    "\n",
    "counter =0 \n",
    "df = pd.DataFrame(columns=all_cols)\n",
    "# get coordinates for all \n",
    "for i in range(len(energies)):\n",
    "#for i in range(1000):\n",
    "    if i%5000==0:\n",
    "        print(f'At entry {i}/{len(energies)}')\n",
    "    data = get_coordinates(i,energies,cols, all_cols)\n",
    "\n",
    "    df = df.append(data, ignore_index = True)\n",
    "\n",
    "\n",
    "energies_new = pd.concat([energies, df], axis=1)\n",
    "energies_new.drop(columns='filename',inplace=True)\n",
    "energies_new.to_csv(path+'energies_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe23a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/energies_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a34309",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a229eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db131cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('data/dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cf897",
   "metadata": {},
   "source": [
    "## END\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
