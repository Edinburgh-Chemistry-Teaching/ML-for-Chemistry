{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d06de947"
   },
   "source": [
    "# Deep Learning for Aqueous Solubility Prediction\n",
    "\n",
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: \n",
    "- Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "- Rohan Gorantla (rohan.gorantla@ed.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48f869de"
   },
   "source": [
    "**About aqueous solubility prediction task**:\n",
    "\n",
    "Water is a ubiquitous solvent in both chemistry and life, so it's not surprising that the solubility of compounds in water is crucial in various fields, such as drug discovery, paint and coatings, the environment, and energy storage. However, measuring and predicting the aqueous solubility of compounds is a challenging and ongoing problem in chemistry. To tackle this, various data-driven prediction models have been created to supplement traditional physics-based methods. Despite the many efforts made over the years, there are still difficulties in developing a solubility prediction model that is accurate enough for many practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDeyesyTOMuG"
   },
   "source": [
    "**Task:** Train a Deep learning model to predict if a given molecule is soluble in water or not. The input data consists of solubility values and 1-D SMILES strings for the molecules. \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j3pliMnCS3C"
   },
   "source": [
    "**Learning Objectives**:\n",
    "\n",
    "* Using RDKit to extract features from molecular data in SMILES format.\n",
    "* Preparing the data for training and testing of a given deep learning model.\n",
    "* To build and train deep neural networks for solubility prediction from SMILES data using Pytorch.\n",
    "* Tuning the neural network parameters to improve the learning of the model for the given solubilitity prediction task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab installs\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following cell install necessary packages and downloads data if you are running this tutorial using Google Colab.<br>\n",
    "<b><i>Run this cell only if you are using Google Colab!</i></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then pip install condacolab; fi\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "import condacolab\n",
    "condacolab.check()\n",
    "!mamba install -c conda-forge mdanalysis mdanalysistests mdanalysisdata nglview scikit-learn ipywidgets=7.6.0 rdkit biopython pysmiles datasmiles \n",
    "!mamba install pytorch torchvision torchinfo -c pytorch\n",
    "\n",
    "# enable third party jupyter widgets\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# copy over data repository\n",
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/Edinburgh-Chemistry-Teaching/ML-for-Chemistry; fi\n",
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then cp -r ML-for-Chemistry/data .; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "196868f4"
   },
   "source": [
    "To get started with this tutorial, let's importing some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBy2P5AVFMiF"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,auc\n",
    "\n",
    "# RDKit\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit.Chem import DataStructs,AllChem\n",
    "from rdkit.Chem import rdmolfiles\n",
    "\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "# import torchvision.transforms as transforms\n",
    "from   torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UT_NLRlMCHEG"
   },
   "source": [
    "## 1. Introduction- Data Prepartion and Exploratory Data Anlaysis\n",
    "\n",
    "In this tutorial we will use the solubility prediciton data from AqSolDB dataset to train the DL models.\n",
    "\n",
    "**Data Source**: Sorkun, M.C., Khetan, A. & Er, S. AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. Sci Data 6, 143 (2019). https://doi.org/10.1038/s41597-019-0151-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPPcf8Z7LXgx"
   },
   "source": [
    "Let's begin by loading the AqSolDB dataset from the csv file in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jIyZPXLeGaVP",
    "outputId": "834f75a1-9f43-4267-80f8-7ef4c5e03a2f"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a dataframe\n",
    "data_path='data'\n",
    "if os.environ.get('COLAB_RELEASE_TAG') is None:\n",
    "    data_path = os.path.join('..','data')\n",
    "    \n",
    "solubility = os.path.join(data_path,'solubility-dataset.csv')\n",
    "df = pd.read_csv(solubility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "l6Hl-DfmPUZ5",
    "outputId": "d5dfb77a-55d0-40ad-f1f2-e9a0a57cdd39"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGO63T-9NSdJ"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1a.</b> From the data frame above, print the number of compounds and list of features in the dataset.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution:</mark> </summary>\n",
    "\n",
    "```Python\n",
    "print(f\"Number of compounds in the dataset: {df.shape[0]}\" )\n",
    "print(f\"Dataset columns: {list(df.columns)}\" )\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XVcB-UJOwnt"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1b.</b> What are the two features that are required in our task to predict solubility of the molecule from SMILES representation ?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSr54y63P191"
   },
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Features:\n",
    "- 'SMILES'\n",
    "- 'Solubility'\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN8EteHYR7XK"
   },
   "source": [
    "The chosen \n",
    "unit of solubility in this dataset is LogS, where S is the aqueous solubility in mol/L (or M). Units such as g/L and mg/L were converted to LogS using the molecular mass of the compounds.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1c.</b> Plot a histogram to analyse the distribution of 'Solubility' measurement across the entire dataset. You can use seaborn histplot for this task. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "BLqOpyzkOkpL",
    "outputId": "5d925de4-3a71-4c03-8683-219e81e546ad"
   },
   "outputs": [],
   "source": [
    "## Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSr54y63P191"
   },
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "sns.histplot(df['Solubility'],bins=20)\n",
    "plt.ylabel('Number of Compunds')\n",
    "plt.show() \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWAbt_6Hf_Qs"
   },
   "source": [
    "Now we want to categorise the Solubility data into two classes-- 0 for insoluble and 1 for soluble compounds. These compounds can be classified according to solubility values (LogS).\n",
    "\n",
    "Compounds with 0 and higher solubility value are highly soluble, those in the range of 0 to −2 are soluble, those in the range of −2 to −4 are slightly soluble and insoluble if less than −4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isSoluble\"] = (df.Solubility > -2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1d.</b> Let's check the number of soluble and not soluble molecules. What happens if you change the cut off? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "df[\"isSoluble\"] = (df.Solubility > -3).astype(int)\n",
    "print(df.isSoluble.value_counts())\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x60As02bhbKT"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1e.</b> How does solubility correlate with molecular weight? Do distributions overlap between compounds that were classified as soluble and not soluble?  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "# Looking at correlation\n",
    "sns.scatterplot(x=\"MolWt\", y='Solubility', data=df[df.MolWt < 1000])\n",
    "# Looking at overlap of soluble and not soluble distributions\n",
    "sns.histplot(x=\"MolWt\", hue='isSoluble', data=df[df.MolWt < 1000], bins=40);\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzVbIVVb9pJB"
   },
   "source": [
    "## 2. Data Preprocessing- Feature extraction and splitting the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnsU50-Qjo3N"
   },
   "source": [
    "Molecular Fingerprinting- ECFP take a look at [J. Chem. Inf. Model. 2010, 50, 5, 742–754](https://pubs.acs.org/doi/10.1021/ci100050t) for more details about molecular fingerprints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb4t8kVs91GI"
   },
   "outputs": [],
   "source": [
    "# Convert SMILES strings to ECFP fingerprints\n",
    "def get_fingerprints(smiles):\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=1024)\n",
    "    arr = np.zeros((1,1024), dtype=np.float32)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_waWEIZPwsR"
   },
   "source": [
    "From the dataframe above, we will now select the input i.e., SMILES string and initialize it to the variable **X** and the binary solubility scores to the variable **y**. We will then get the Morgan fingerprints for the input **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfdevnXUrs3_"
   },
   "outputs": [],
   "source": [
    "X = df[\"SMILES\"]\n",
    "y=np.array(df[\"isSoluble\"]) # label\n",
    "#Get ECFP Fingerprints\n",
    "X1 = X.apply(get_fingerprints).apply(lambda x: np.array(x, dtype=np.float32))\n",
    "X1 = np.concatenate(X1.values)\n",
    "X1 = X1.reshape(-1, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have **X1** with fingerprints for all the molecules in the dataset and corresponding solubility scores. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2.</b> Explore what you see in X1 in ab it more detail! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "For example you can look at the shape or number of 1s and 0s in your feature vector\n",
    "    \n",
    "```Python\n",
    "np.shape(X1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn9HWpOwjtaS"
   },
   "source": [
    "Now we have **X1** with fingerprints for all the molecules in the dataset and corresponding solubility scores. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2a.</b> Split X and y into train, test and validation sets using scikit-learn. Use 80% data for training and 20% data for testing; You can use 20% of the training data for validation set. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X1 is the training data and our y is the label\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(# your solution here\n",
    "\n",
    "# Split the train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = # your soluion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "    \n",
    "```Python\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.2)\n",
    "\n",
    "# Split the train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlR27GU1T__M"
   },
   "source": [
    "We will now have to convert the `train`, `val` and `test` sets into `tensors` and then use PyTorch `DataLoader` to wrap these samples for training the DL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4F0dCRBTq1T"
   },
   "outputs": [],
   "source": [
    "# Convert your data into tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
    "\n",
    "# Wrap your data in TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6X51dvnUqDJ"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2b.</b> Similar to the train set as shown in cell above, prepare the testloader and validation loader. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your data into tensors\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = \n",
    "X_test = \n",
    "y_test = \n",
    "\n",
    "# Wrap your data in TensorDataset\n",
    "val_dataset = \n",
    "test_dataset = \n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "valloader = DataLoader()\n",
    "testloader = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "    \n",
    "```Python\n",
    "# Convert your data into tensors\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.int64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Wrap your data in TensorDataset\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset= TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkqBtnE7DTyo"
   },
   "outputs": [],
   "source": [
    "# Convert your data into tensors\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.int64)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
    "\n",
    "# Wrap your data in TensorDataset\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset= TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tocfZx3DWwy"
   },
   "source": [
    "## 3. Training the Deep Neural Network on SMILES data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdEx3l9KJGfb"
   },
   "source": [
    "### 3.1. Defining the DNN model\n",
    "Now let's get stated by defining a 5-layer deep neural network which takes in a input a 1024 dimensions ECFP fingerprint obtained from the corresponding SMILES string. \n",
    "\n",
    "This code defines a class named DNN which extends PyTorch's `nn.Module` class. The DNN class is used to build a deep neural network model. The class has two main parts: the constructor `__init__` and the `forward` method.\n",
    "\n",
    "In the constructor, we first call the `super()` method to initialize the parent class `nn.Module`. Then, we create five fully connected linear layers using `nn.Linear` from the PyTorch's nn module. The first layer `fc1` has 1024 inputs and 512 outputs, the second layer `fc2` has 512 inputs and 256 outputs, the third layer `fc3` has 256 inputs and 128 outputs, the fourth layer `fc4` has 128 inputs and 64 outputs, and the final layer `fc5` has 64 inputs and 2 outputs.\n",
    "\n",
    "In the forward method, we define the forward pass of the network. We first apply the `F.relu` activation function to the outputs of each layer, which applies the rectified linear unit (ReLU) activation function. After applying activation, we apply the `F.softmax `function to the output of the final layer to get probabilities for each class. The final output of the forward method is the probability distribution over the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vj7a0gpmBYis"
   },
   "outputs": [],
   "source": [
    "# Define the DNN model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.softmax(self.fc5(x), dim=1) # Use softmax activation to get probabilities for each class\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_HeEwK3Ze4b"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3a.</b> What is an activation function? Why do you think we need an activation function in a deep neural network? Give some examples of activation functions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvEzFFjvZ7A6"
   },
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "An activation function is a non-linear function applied to the output of each neuron in a deep neural network. It transforms the linear combination of inputs and weights into a non-linear representation, which allows the neural network to learn non-linear relationships in the data.\n",
    "\n",
    "Examples: Sigmoid, ReLU, Tanh, Leaky ReLU, etc.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQp7tylvZDET"
   },
   "source": [
    "Next we will intialize the `DNN` model and define the loss function and optimizer. Loss function is used to measure whether the model is doing a good job of learning the training data. It is then averaged over every sample in the training set.\n",
    "\n",
    "Now that we have a way to measure how well the model works, we need a way to improve it. We want to search for parameter values that minimize the average loss over the training set in each step, for this we need an optimiser.  The learning rate `lr` determines how much the parameters change on each step.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4i2VSafr3Fu"
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = DNN()\n",
    "criterion = nn.CrossEntropyLoss() # Use cross entropy loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Use Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K_OKND8cIwm"
   },
   "source": [
    "Let's see the number of parameters that are present in the simple DNN we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6aipxWKwLhY",
    "outputId": "414573a9-dc11-4b6d-859e-400a3ddcc590"
   },
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChhrKWpGctdV"
   },
   "source": [
    "### 3.2. Training the DNN model\n",
    "Now let's start training the DNN model. We will train this model over 50 epochs and save the training loss and validation loss to monitor the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCkrQmsEstEX"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.long()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(trainloader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            val_batch_loss = criterion(outputs, labels.long()) \n",
    "            val_running_loss += val_batch_loss.item()\n",
    "        \n",
    "    avg_val_loss = val_running_loss / len(valloader)\n",
    "    val_loss.append(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qupdjTuVdG5Y"
   },
   "source": [
    "**Training loss** and **validation loss** are metrics used in deep learning to evaluate the performance of a model during training.\n",
    "\n",
    "Training loss measures the error between the predicted output and the true output on the training data. It helps to determine how well the model is learning from the training data and whether it is overfitting or underfitting. If the training loss is decreasing, it means the model is learning and improving.\n",
    "\n",
    "Validation loss, on the other hand, measures the error between the predicted output and the true output on a validation dataset, which is separate from the training data. The validation loss provides an estimate of the model's generalization ability, that is, its ability to perform well on new, unseen data. A high validation loss indicates overfitting to the training data, which means the model is not generalizing well to new data.\n",
    "\n",
    "In general, monitoring both the training loss and the validation loss is important for guiding the training process and selecting a well-performing model. Lets see how the `DNN` model learnt over 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "K-Oaj2pmD115",
    "outputId": "d73f8c01-4eb2-4f3a-bf61-c6daa78e2673"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.plot(range(num_epochs), train_loss, label='Training Loss')\n",
    "plt.plot(range(num_epochs), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Fqd7GuBeUbR"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3b.</b> From the above plot, what do you think about the model's learning over 50 epochs. Comment.[link text](https://) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfM4eQOgdxM3"
   },
   "source": [
    "### 3.3. Testing the DNN model\n",
    "\n",
    "Model testing is an important step after training a machine learning model because it provides an estimate of how well the model is likely to perform on unseen data. This is important because a model that performs well on the training data may not necessarily perform well on unseen data, which can result in poor performance and unreliable results. Testing helps to identify overfitting, which occurs when a model is too complex and has learned the training data too well, causing it to perform poorly on new data. Model testing also helps to estimate the generalization error, which is a measure of how well the model is expected to perform on unseen data. By testing the model, we can evaluate its performance and identify any areas where the model needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOR5i6PwhrX"
   },
   "outputs": [],
   "source": [
    "# Test the model on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, target = data\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the class with the highest probability\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.long()).sum().item() # Convert target to long and compare\n",
    "        predictions += predicted.tolist()\n",
    "        true_labels += target.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb9dk2TvePI4"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3c.</b> We have calculated above the correct predictions. Can you print the accuracy of the model in % on test data using the variables defined above? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twBTv55Ne2Zb"
   },
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "\n",
    "```Python\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the network on the test data: {(100 * accuracy):.2f}%'))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h-3DycVfTbj"
   },
   "source": [
    "Lets define a confusion matrix. It is a plot that is used to evaluate the performance of a classification algorithm. It displays the number of correct and incorrect predictions made by the algorithm in a tabular format. The matrix is used to determine the true positive rate, false positive rate, accuracy, precision, and recall, which are all metrics that help in evaluating the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "7YaPRbax3-tZ",
    "outputId": "0456680e-b1a1-4fcc-97f2-1523a59d996b"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cf_matrix= confusion_matrix(true_labels, predictions)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXtlcc44gCFX"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3d.</b> From the confusion matrix comment on the performance on the test set. Do you think the model is underfitting or overfitting?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receiver Operator Characteristic (ROC) curve are a good way to assess how good your binary classifier is versus random guessing. \n",
    "Take a look [here](https://dmol.pub/ml/classification.html#receiver-operating-characteristic-curve) for more information on ROC curves. And here some information on how to compute these with [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak4P9KcBfkYl"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3e.</b> Plot an ROC curve from <code>sklearn</code> using true_labels and predictions defined above.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--') # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKfqSIxpUcBz"
   },
   "source": [
    "### 3.4. Modeling choices and Hyperparameter Optimization\n",
    "\n",
    "By now you have probaly noticed that there are a lot of choices to make, even when using a supposedly simple DL model with a “generic” learning algorithm. Examples include:\n",
    "- The number of layers in the model\n",
    "- The loss function\n",
    "- The number of training steps to perform\n",
    "- The learning rate to use during *training*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSkmYPgUgluI"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3f.</b> Using the code above retrain a new DNN model with differnt learning rates for example 0.1 and 0.0001. Compare the performances of these models. Plot confusion matrices for both these learning rates and comment on the model fits.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLc4fr7ZhWLz"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3g.</b> Using the code above retrain a new DNN model for 100 and 200 epochs. Does this help in model's learning? Plot confusion matrices for both these epoch settings and comment on the model fits.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmdjlF9WhVHC"
   },
   "outputs": [],
   "source": [
    "# Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsURtvCzg5iR"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3h.</b> Using the code above define a new DNN model architecture called `DNN_new` with 3 layers. Plot confusion matrices for the new model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbdOv-xRg4tj"
   },
   "outputs": [],
   "source": [
    "# Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cQ87H_7cV9p"
   },
   "source": [
    "## 4. Extra Topic: Convolutional Neural Networks\n",
    "\n",
    "We define a new Deep Learning variant called **1D Convolutional Neural Network (CNN)** below.\n",
    "CNN is a type of deep learning model that is typically used for image and signal processing tasks. \n",
    "\n",
    "In this code, the model takes a 1D input signal of length 1024 and performs a series of convolution, pooling and fully connected operations to classify the signal into one of two classes. The `conv1` and `conv2` layers are 1D convolutional layers that apply filters to the input signal to extract features. The `pool1` and `pool2` layers are max-pooling layers that down-sample the input signal. The `fc1` and `fc2` layers are fully connected layers that perform classification based on the extracted features. The `softmax` activation is applied to the output to produce probabilities for each class. This gives us the classification into soluble and insoluble molecules. \n",
    "\n",
    "The steps are similar to DNN model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrYS2bQQcgIK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the 1D CNN model\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 256, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 1024)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.reshape(-1, 32 * 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvKYFLNjcgiR"
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model_cnn = CNN1D()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEyYsKHGcp_L",
    "outputId": "9e33abda-9090-4135-a9c1-d4b1df4ded35"
   },
   "outputs": [],
   "source": [
    "summary(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Fp9iXSJcuaP",
    "outputId": "98535ad4-ded3-4c36-9914-e2636ade3d9c"
   },
   "outputs": [],
   "source": [
    "# Train the CNN model and keep track of validation loss\n",
    "train_loss1 = []\n",
    "val_loss1 = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_cnn(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(trainloader)\n",
    "    train_loss1.append(avg_train_loss)\n",
    "    \n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            inputs, labels = data\n",
    "            outputs = model_cnn(inputs)\n",
    "            \n",
    "            val_batch_loss = criterion(outputs, labels.long())\n",
    "            val_running_loss += val_batch_loss.item()\n",
    "        \n",
    "    avg_val_loss = val_running_loss / len(valloader)\n",
    "    val_loss1.append(avg_val_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i-2mk-iedcU"
   },
   "outputs": [],
   "source": [
    "# Plot the validation loss\n",
    "plt.plot(range(num_epochs), train_loss1, label='Training Loss')\n",
    "plt.plot(range(num_epochs), val_loss1, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6FUrrNhiWKs"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 4a.</b> From the above plot, what do you think about the CNN model's learning over 50 epochs. Compare its performance with DNN. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4HCwF2vkE09"
   },
   "outputs": [],
   "source": [
    "# Test the model on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        inputs, target = data\n",
    "        outputs = model_cnn(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the class with the highest probability\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.long()).sum().item() # Convert target to long and compare\n",
    "        predictions += predicted.tolist()\n",
    "        true_labels += target.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaMGKBBbIOqL"
   },
   "outputs": [],
   "source": [
    "accuracy = correct / total\n",
    "print(f'Accuracy of the network on the test data: {(100 * accuracy):.2f} %' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtC05cwuISrL"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cf_matrix= confusion_matrix(true_labels, predictions)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cf_matrix.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruKlDjVcipAP"
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 4b.</b> Is there a change in true positive, false positive and false negative rates as compared to the DNN model? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END\n",
    "----------"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
