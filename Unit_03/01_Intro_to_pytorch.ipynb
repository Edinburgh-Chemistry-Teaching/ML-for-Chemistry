{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73a72cab",
   "metadata": {},
   "source": [
    "# Getting Started with PyTorch\n",
    "    \n",
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: \n",
    "- Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "- Katerina Karoni\n",
    "\n",
    "Some content was also adapted from the [scikit-learn](https://scikit-learn.org/stable/auto_examples/index.html) and [pytorch](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) documentation and examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96047d0",
   "metadata": {},
   "source": [
    "**Learning Objectives**:\n",
    "* Getting familiar with some PyTorch basics:\n",
    "    * What is a tensor\n",
    "    * How to build a dataset\n",
    "    * How to build a neural network\n",
    "    * Choosing an optimiser\n",
    "    * Testing and training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33497e6",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "* to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "* to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97feb8",
   "metadata": {},
   "source": [
    "## PyTorch Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58247d",
   "metadata": {},
   "source": [
    "PyTorch is a Python framework that provides two high-level features:\n",
    "\n",
    "    Tensor computation (like NumPy) with strong GPU acceleration\n",
    "    Deep neural networks built on a autograd system\n",
    "\n",
    "You can reuse your favorite Python packages such as NumPy to extend PyTorch when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5215a1b",
   "metadata": {},
   "source": [
    "**<h2>Pytorch Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0317ce5",
   "metadata": {},
   "source": [
    "So far we have only used scikit-learn. Today we will be using PyTorch. If you wanted to install it locally then use the command below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f601b",
   "metadata": {},
   "source": [
    "```conda install pytorch torchvision -c pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff5d68",
   "metadata": {},
   "source": [
    "For more information on the installation please check out: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22931231",
   "metadata": {},
   "source": [
    "⚠️ If you want to make use of the GPUs on Colab we should use the Colab version of today's notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb3391",
   "metadata": {},
   "source": [
    "## Google Colab package installs\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following cell install necessary packages and downloads data if you are running this tutorial using Google Colab.<br>\n",
    "<b><i>Run this cell only if you are using Google Colab!</i></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98976c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then pip install condacolab; fi\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "\n",
    "import condacolab\n",
    "condacolab.check()\n",
    "!mamba install -c conda-forge scikit-learn\n",
    "!mamba install pytorch torchvision torchinfo -c pytorch\n",
    "\n",
    "# copy over data repository\n",
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/Edinburgh-Chemistry-Teaching/ML-for-Chemistry; fi\n",
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then cp -r ML-for-Chemistry/data .; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9bed8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7db7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53553e38",
   "metadata": {},
   "source": [
    "**<h2>Tensors**\n",
    "    \n",
    "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
    "\n",
    "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either **CPU** or **GPU**. To run operations on the GPU, just cast the Tensor to a `cuda datatype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56430451",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(f'x = {x}')\n",
    "\n",
    "print(f'Size of x = {x.size()}') # np.shape(x) also works\n",
    "\n",
    "print(f'Data type of x = {x.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also specify data type\n",
    "\n",
    "y = torch.zeros(2,3,dtype=torch.float32) \n",
    "\n",
    "print(f'y = {y}')\n",
    "\n",
    "print(f'Data type of y = {y.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271cf66",
   "metadata": {},
   "source": [
    "**Casting tensor x as cuda datatype if cuda available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c07389",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b80cf",
   "metadata": {},
   "source": [
    "Attention: ```x.to(device)``` will not cast ```x```as cuda datatype - we need\n",
    "```x = x.to(device)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e3d99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = x.to(device) # or x = x.cuda() for GPU\n",
    "print(x.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1ebc0",
   "metadata": {},
   "source": [
    "Note: For tensors ```x.to(device)```, as mentioned does not move ```x``` to cuda and we need to write ```x = x.to(device)``` instead.\n",
    "\n",
    "However, for neural networks, ```net.to(device)``` and\n",
    "```net = net.to(device)``` are equivalent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f5254",
   "metadata": {},
   "source": [
    "**Tensor Data types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad495d27",
   "metadata": {},
   "source": [
    "```DoubleTensor``` is ```64-bit``` floating point and ```FloatTensor``` is ```32-bit``` floating point tensor. So a ```FloatTensor``` uses half of the memory as a same tensor-size ```DoubleTensor``` uses. Also GPU and CPU computations with lower precision are much faster.  However, if high precision is needed, go for ```DoubleTensor``` . So Pytorch leaves it to user to choose which one to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5458a",
   "metadata": {},
   "source": [
    "Set default tensor type for your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009acf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')    # 32 bits\n",
    "#or\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')    # 64 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(2,3) \n",
    "print(x)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037decaa",
   "metadata": {},
   "source": [
    "## Show case: Components needed to run a classification with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4bfe2",
   "metadata": {},
   "source": [
    "We will now present an example where we train a fully-connected neural network on a popular benchmark dataset: MNIST handwritten digits.\n",
    "\n",
    "The MNIST had-written digits dataset consists of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 image and an associated label from one of 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn import datasets\n",
    "from   torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413d6d9",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557735be",
   "metadata": {},
   "source": [
    "The ```torchvision``` library consists of popular datasets, model architectures, and common image transformations for computer vision. Since we have been using `scikit learn` we will use this data set in the example here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ffd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7020ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Training: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12249f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a torch tensor:\n",
    "\n",
    "x = torch.Tensor(digits['data']) # data\n",
    "y = torch.Tensor(digits['target']) # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ecb35",
   "metadata": {},
   "source": [
    "**<h3> Dataloader class**\n",
    "    \n",
    "https://pytorch.org/docs/stable/data.html#map-style-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f955aa",
   "metadata": {},
   "source": [
    "At the heart of ```PyTorch``` data loading utility is the ```torch.utils.data.DataLoader``` class. It represents a Python iterable over a dataset\n",
    "\n",
    "```python\n",
    "DataLoader(dataset, batch_size=1, shuffle=False, \n",
    "           sampler=None,batch_sampler=None, num_workers=0,\n",
    "           collate_fn=None,pin_memory=False, drop_last=False, \n",
    "           timeout=0, worker_init_fn=None, *, prefetch_factor=2,\n",
    "           persistent_workers=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96636f",
   "metadata": {},
   "source": [
    "The most important argument of ```DataLoader``` constructor is ```dataset```, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\n",
    "\n",
    "- Map style datasets: Datasets that implement the ```__getitem__()``` and ```__len__()``` methods and are maps from keys to data samples.\n",
    "\n",
    "- Iterable style datasets: When reading from a stream of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# Split the train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60779ddb",
   "metadata": {},
   "source": [
    "Let's check that our data is of torch data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next the data needs to be wrapped into a Tensor dataset and then we can use a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeccfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap your data in TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size = 128\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape train and test images from 8x8 to 1x64\n",
    "for i,data in enumerate(dataloader_train):\n",
    "    xtrain = data[0].view(-1,64)       # torch.Tensor.view() is equivalent to reshape\n",
    "    ytrain = data[1].type(torch.LongTensor)                    # train labels\n",
    "\n",
    "for i,data in enumerate(dataloader_test):      \n",
    "    xtest = data[0].view(-1,64)     # test images, The size -1 is inferred from other dimensions\n",
    "    ytest = data[1].type(torch.LongTensor)                            # test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b654ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_mnist_train       = TensorDataset(xtrain, ytrain)\n",
    "flat_dataloader_train  =  DataLoader(flat_mnist_train, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0faaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fee349",
   "metadata": {},
   "source": [
    "## Defining the Neural network\n",
    "\n",
    "This is a 4 layer linear fully connected network. This is just an example\n",
    "Notice how the forward function is defined and how it uses the ReLu activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555148b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_net(nn.Module):    # this class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(Neural_net, self).__init__() #this calls the constructor of the parent class nn.Module\n",
    "        \n",
    "        # define network layers\n",
    "        self.fc1 = nn.Linear(64, 20)   # nn.Linear is a class for linear layers (16,12) are the constructor arguments\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 20)\n",
    "        self.fc4 = nn.Linear(20, 10)\n",
    "        torch.manual_seed(4)           # generating numbers changes the state of the random number generator.\n",
    "                                       # we thus have to set the seed back to 2  \n",
    "            \n",
    "        # Notice the RELU activation function below\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))  # \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f452c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_accuracy(output,y):\n",
    "    '''If np.argmax(out,axis=1)-y is non-zero, i.e. label y is 9 and prediction is 5 then \n",
    "       diff is incremented by 1, i.e. every time there is a mismatch between prediction and label.\n",
    "       The ratio diff/np.size(y) gives us the ratio of false predictions over the total number of datapoints.\n",
    "       One minus that gives the model accuracy.\n",
    "       '''\n",
    "    # we can't call numpy() on Tensors that requires grad. So, in order to compute diff (see below)\n",
    "    # we need to use tensor.detach().numpy()\n",
    "    output = output.cpu().detach().numpy()    # no need for the .cpu() here as we are working with cpu tensors\n",
    "    y      = y.cpu().detach().numpy()\n",
    "    diff   = np.count_nonzero(np.argmax(output,axis=1)-y) # np.argmax returns the index/indices of max value(s)\n",
    "                                                        # along specified axis\n",
    "    return (1-(diff/np.size(y)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039aca31",
   "metadata": {},
   "source": [
    "## Important Model and training parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1425302",
   "metadata": {},
   "source": [
    "### The optimiser\n",
    "\n",
    "The optimiser module gives access to a large number of standard optimisers that try and help minimise the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd67598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2a611",
   "metadata": {},
   "source": [
    "There are different options. Take a look at adam and sgd (stocastic gradient descent), both are very common choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cdd7ca",
   "metadata": {},
   "source": [
    "### The loss\n",
    "\n",
    "Typically for a classification problem one would use a cross entrop loss, the [torch documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) has some more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff51795",
   "metadata": {},
   "source": [
    "Cross entropy loss is generally defined for two probability distributions $p$ and $q$ as:\n",
    "\n",
    "$$H(p,q) =  –\\sum_{x \\in \\mathcal{X}} p(x)  \\log(q(x))$$\n",
    "\n",
    "For binary classification\n",
    "\n",
    "$$\\mathrm{loss}=−(y \\log(p)+(1−y) \\log(1−p))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b4de0",
   "metadata": {},
   "source": [
    "### The learning rate \n",
    "This is a parameter you set in your optimiser an you can play around with different values for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aedfb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# The criterion we want to optimse\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining the optimiser\n",
    "# lr is the learnig rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=5*10**-3)\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for i,data in enumerate(flat_dataloader_train):\n",
    "        # Load in the training datapoints\n",
    "        x=data[0]                 \n",
    "        y=data[1]   \n",
    "        optimizer.zero_grad()\n",
    "        # The output of the current net\n",
    "        output = net(x)\n",
    "        loss = criterion(output,y) \n",
    "        loss.backward()\n",
    "        # Optimising one more step\n",
    "        optimizer.step()\n",
    "                   \n",
    "        if epoch % 1 == 0 and i==0:\n",
    "            # test the accuracy \n",
    "            acc        = class_accuracy(output,y)\n",
    "            outputtest = net(xtest)\n",
    "            loss_test  = criterion(outputtest,ytest)\n",
    "            acc_test   = class_accuracy(outputtest,ytest)\n",
    "            print(f'epoch {epoch}/{num_epochs}, accuracy train {acc:.2f} %, loss train, {loss.item():.5f}, accuracy test {acc_test:.2f} %, loss test, {loss_test.item():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2bc56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task:</b> Train the neural network using 100 epochs and plot the quantities printed to screen.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23925b",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "# Let's reinisalise the network\n",
    "net = Neural_net()\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# The criterion we want to optimse\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining the optimiser\n",
    "# lr is the learnig rate\n",
    "optimizer = optim.Adam(net.parameters(), lr=5*10**-3)\n",
    "\n",
    "# list where we want to store the training information\n",
    "info = []\n",
    "for epoch in range(num_epochs): \n",
    "    for i,data in enumerate(flat_dataloader_train):\n",
    "        # Load in the training datapoints\n",
    "        x=data[0]                 \n",
    "        y=data[1]   \n",
    "        optimizer.zero_grad()\n",
    "        # The output of the current net\n",
    "        output = net(x)\n",
    "        loss = criterion(output,y) \n",
    "        loss.backward()\n",
    "        # Optimising one more step\n",
    "        optimizer.step()\n",
    "                   \n",
    "        if epoch % 1 == 0 and i==0:\n",
    "            # test the accuracy\n",
    "            acc        = class_accuracy(output,y)\n",
    "            outputtest = net(xtest)\n",
    "            loss_test  = criterion(outputtest,ytest).detach().cpu().numpy()\n",
    "            acc_test   = class_accuracy(outputtest,ytest)\n",
    "            print(float(loss_test))\n",
    "            info.append([acc, float(loss_test),acc_test])\n",
    "            print(f'epoch {epoch}/{num_epochs}, accuracy train {acc:.2f} %, loss train, {loss.item():.5f}, accuracy test {acc_test:.2f} %, loss test, {loss_test.item():.5f}')\n",
    "info = np.array(info)\n",
    "epochs = np.linspace(0,num_epochs,len(info))\n",
    "plt.plot(epochs, info[:,0], label='accuracy')\n",
    "plt.plot(epochs, info[:,1], label='loss_test')\n",
    "plt.plot(epochs, info[:,2], label='accuracy test')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9549c91",
   "metadata": {},
   "source": [
    "## END\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
